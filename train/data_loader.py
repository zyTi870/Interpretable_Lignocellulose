import os
import glob
from collections import defaultdict
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from history.cell_fiber_dataset import CellFiberCleanDataset

def get_data_loaders(data_root, batch_size=12, num_workers=8):
    """
    è‡ªåŠ¨è¯»å– data_root ä¸‹çš„å­æ–‡ä»¶å¤¹ä½œä¸ºåˆ†ç±»ç±»åˆ«
    ä¿®æ”¹ï¼šåˆ’åˆ†æ—¶ç¡®ä¿åŒä¸€åŸå§‹æ ·æœ¬çš„æ‰€æœ‰æ‰©å……æ•°æ®ï¼ˆæ—‹è½¬/è£å‰ªï¼‰éƒ½åœ¨åŒä¸€ä¸ªé›†ä¸­
    """
    if not os.path.exists(data_root):
        raise ValueError(f"Data root not found: {data_root}")

    # --- 1. è‡ªåŠ¨æ£€æµ‹ç±»åˆ« ---
    CLASSES = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]
    CLASSES.sort()

    if len(CLASSES) < 2:
        print(f"âš ï¸  ä¸¥é‡è­¦å‘Š: ä»…æ£€æµ‹åˆ° {len(CLASSES)} ä¸ªç±»åˆ«: {CLASSES}ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")

    class_mapping = {cls_name: i for i, cls_name in enumerate(CLASSES)}
    print(f"âœ… æ£€æµ‹åˆ°çš„ç±»åˆ« (è‡ªåŠ¨): {CLASSES}")
    print(f"âœ… ç±»åˆ«æ˜ å°„è¡¨: {class_mapping}")

    # --- 2. è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„ ---
    all_files = []
    for cls_name in CLASSES:
        cls_path = os.path.join(data_root, cls_name)
        files = glob.glob(os.path.join(cls_path, "*.npz"))
        all_files.extend(files)
        print(f"   - ç±»åˆ« '{cls_name}': å‘ç° {len(files)} ä¸ªæ–‡ä»¶")

    if len(all_files) == 0:
        raise ValueError(f"åœ¨ {data_root} ä¸‹æœªæ‰¾åˆ°ä»»ä½• .npz æ–‡ä»¶")

    # --- 3. æ ‡ç­¾æå–ä¸åˆ’åˆ† (æ ¸å¿ƒä¿®æ”¹) ---
    # ç­–ç•¥ï¼šå…ˆæŒ‰ç…§æ–‡ä»¶åç‰¹å¾å°†å±äºåŒä¸€ä¸ªæ ·æœ¬çš„æ–‡ä»¶å½’ç±»ï¼Œç„¶åæŒ‰æ ·æœ¬IDè¿›è¡Œåˆ’åˆ†

    sample_groups = defaultdict(list)
    sample_labels_map = {}

    for f in all_files:
        basename = os.path.basename(f)
        # æå–æ ·æœ¬ID:
        # å‡è®¾æ–‡ä»¶åä¸º E33_rot000_BL.npzï¼Œåˆ™é€šè¿‡åˆ†å‰² '_rot' è·å–å‰ç¼€ 'E33' ä½œä¸ºå”¯ä¸€ID
        if '_rot' in basename:
            sample_id = basename.split('_rot')[0]
        else:
            # å…¼å®¹ä¸åŒ…å« _rot çš„æƒ…å†µï¼Œé»˜è®¤å–ç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿å‰çš„å†…å®¹
            sample_id = basename.split('_')[0]

        sample_groups[sample_id].append(f)

        # è·å–è¯¥æ ·æœ¬çš„ç±»åˆ« (çˆ¶æ–‡ä»¶å¤¹å)
        label = os.path.basename(os.path.dirname(f))
        sample_labels_map[sample_id] = label

    # æå–å”¯ä¸€çš„æ ·æœ¬IDåˆ—è¡¨å’Œå¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨ç”¨äºåˆ†å±‚åˆ’åˆ†
    unique_ids = list(sample_groups.keys())
    unique_labels = [sample_labels_map[uid] for uid in unique_ids]

    print(f"ğŸ” è¯†åˆ«åˆ° {len(unique_ids)} ä¸ªç‹¬ç«‹åŸå§‹æ ·æœ¬ (æ‰©å……å‰)")

    # åˆ’åˆ†: Train(80%) / Val(10%) / Test(10%) åŸºäº æ ·æœ¬ID
    # ç¬¬ä¸€æ­¥ï¼šåˆ‡åˆ†å‡º Test (10% çš„æ ·æœ¬ID)
    train_val_ids, test_ids, train_val_labels, test_labels = train_test_split(
        unique_ids, unique_labels, test_size=0.1, stratify=unique_labels, random_state=42
    )

    # ç¬¬äºŒæ­¥ï¼šä»å‰©ä½™çš„æ ·æœ¬IDä¸­åˆ‡åˆ†å‡º Val (æ€»é‡çš„10%)
    train_ids, val_ids, train_labels, val_labels = train_test_split(
        train_val_ids, train_val_labels, test_size=1/9, stratify=train_val_labels, random_state=42
    )

    # å°†åˆ’åˆ†å¥½çš„ ID åˆ—è¡¨è¿˜åŸä¸ºå¯¹åº”çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    def flatten_files(id_list):
        files = []
        for uid in id_list:
            files.extend(sample_groups[uid])
        return files

    train_files = flatten_files(train_ids)
    val_files = flatten_files(val_ids)
    test_files = flatten_files(test_ids)

    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ† (æŒ‰æ ·æœ¬ID) -> Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}")
    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ† (æ€»æ–‡ä»¶æ•°) -> Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}")

    # --- 4. åˆ›å»º DataLoader ---
    train_dataset = CellFiberCleanDataset(train_files, class_mapping=class_mapping)
    val_dataset = CellFiberCleanDataset(val_files, class_mapping=class_mapping)
    test_dataset = CellFiberCleanDataset(test_files, class_mapping=class_mapping)

    # é’ˆå¯¹ A800 å¼€å¯ pin_memory åŠ é€Ÿ
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return train_loader, val_loader, test_loader